{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95d1f1e-e4cc-4c04-a3fd-5796bc23252e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers import evaluation\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c11f0b-043c-44a3-96bf-3c447c4e1703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../shopping_queries_dataset/'\n",
    "locale =\"us\"\n",
    "model_save_path = f\"./models_{locale}\"\n",
    "output_path = f\"{model_save_path}_tmp\"\n",
    "random_state = 42\n",
    "n_dev_queries = 200\n",
    "train_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9e6741-a2e0-4f84-9e0b-c6681ec4695f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "---------> cuda is activated <----------\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 0. Init variables \"\"\"\n",
    "col_query = \"query\"\n",
    "col_query_id = \"query_id\"\n",
    "col_product_id = \"product_id\" \n",
    "col_product_title = \"product_title\"\n",
    "col_product_locale = \"product_locale\"\n",
    "col_esci_label = \"esci_label\" \n",
    "col_small_version = \"small_version\"\n",
    "col_split = \"split\"\n",
    "col_gain = 'gain'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "to_print = \"\".join(['-']*40)\n",
    "print(to_print)\n",
    "print(f\"---------> {device} is activated <----------\")\n",
    "print(to_print)\n",
    "esci_label2gain = {\n",
    "    'E' : 1.0,\n",
    "    'S' : 0.1,\n",
    "    'C' : 0.01,\n",
    "    'I' : 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4072628b-f764-4e67-b164-55f0b7a9919a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" 1. Load data \"\"\"    \n",
    "df_examples = pd.read_parquet(os.path.join(dataset_path, 'shopping_queries_dataset_examples.parquet'))\n",
    "df_products = pd.read_parquet(os.path.join(dataset_path, 'shopping_queries_dataset_products.parquet'))\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=[col_product_locale, col_product_id],\n",
    "    right_on=[col_product_locale, col_product_id]\n",
    ")\n",
    "df_examples_products = df_examples_products[df_examples_products[col_small_version] == 1]\n",
    "df_examples_products = df_examples_products[df_examples_products[col_split] == \"train\"]\n",
    "df_examples_products = df_examples_products[df_examples_products[col_product_locale] == locale]\n",
    "df_examples_products[col_gain] = df_examples_products[col_esci_label].apply(lambda esci_label: esci_label2gain[esci_label])\n",
    "\n",
    "list_query_id = df_examples_products[col_query_id].unique()\n",
    "dev_size = n_dev_queries / len(list_query_id)\n",
    "list_query_id_train, list_query_id_dev = train_test_split(list_query_id, test_size=dev_size, random_state=random_state)\n",
    "\n",
    "df_examples_products = df_examples_products[[col_query_id, col_query, col_product_title, col_gain]]\n",
    "df_train = df_examples_products[df_examples_products[col_query_id].isin(list_query_id_train)]\n",
    "df_dev = df_examples_products[df_examples_products[col_query_id].isin(list_query_id_dev)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c39fa9a-308b-46b1-8476-54ce42ba3751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966db40a2d234b10b2db082d7768ca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8b53b1bf284261a35b6d6cca3135ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12989 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" 2. Prepare data loaders \"\"\"\n",
    "train_samples = []\n",
    "for (_, row) in df_train.iterrows():\n",
    "    train_samples.append(InputExample(texts=[row[col_query], row[col_product_title]], label=float(row[col_gain])))\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size, drop_last=True)\n",
    "if locale == \"us\":\n",
    "    dev_samples = {}\n",
    "    query2id = {}\n",
    "    for (_, row) in df_dev.iterrows():\n",
    "        try:\n",
    "            qid = query2id[row[col_query]]\n",
    "        except KeyError:\n",
    "            qid = len(query2id)\n",
    "            query2id[row[col_query]] = qid\n",
    "        if qid not in dev_samples:\n",
    "            dev_samples[qid] = {'query': row[col_query], 'positive': set(), 'negative': set()}\n",
    "        if row[col_gain] > 0:\n",
    "            dev_samples[qid]['positive'].add(row[col_product_title])\n",
    "        else:\n",
    "            dev_samples[qid]['negative'].add(row[col_product_title])\n",
    "    evaluator = CERerankingEvaluator(dev_samples, name='train-eval')\n",
    "\n",
    "    \"\"\" 3. Prepare Cross-enconder model:\n",
    "        https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/ms_marco/train_cross-encoder_kd.py\n",
    "    \"\"\"\n",
    "    model_name = 'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "    num_epochs = 1\n",
    "    num_labels = 1\n",
    "    max_length = 512\n",
    "    default_activation_function = torch.nn.Identity()\n",
    "    model = CrossEncoder(\n",
    "        model_name, \n",
    "        num_labels=num_labels, \n",
    "        max_length=max_length, \n",
    "        default_activation_function=default_activation_function, \n",
    "        device=device\n",
    "    )\n",
    "    loss_fct=torch.nn.MSELoss()\n",
    "    evaluation_steps = 5000\n",
    "    warmup_steps = 5000\n",
    "    lr = 7e-6\n",
    "    \"\"\" 4. Train Cross-encoder model \"\"\"\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        loss_fct=loss_fct,\n",
    "        evaluator=evaluator,\n",
    "        epochs=num_epochs,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=output_path,\n",
    "        optimizer_params={'lr': lr},\n",
    "    )\n",
    "    model.save(model_save_path)\n",
    "else:\n",
    "    dev_queries = df_dev[col_query].to_list()\n",
    "    dev_titles = df_dev[col_product_title].to_list()\n",
    "    dev_scores = df_dev[col_gain].to_list()   \n",
    "    evaluator = evaluation.EmbeddingSimilarityEvaluator(dev_queries, dev_titles, dev_scores)\n",
    "\n",
    "    \"\"\" 3. Prepare sentence transformers model: \n",
    "        https://www.sbert.net/docs/training/overview.html \n",
    "    \"\"\"\n",
    "    model_name = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "    num_epochs = 1\n",
    "    evaluation_steps = 1000\n",
    "    \"\"\" 4. Train Sentence transformer model \"\"\"\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=num_epochs,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        output_path=output_path,\n",
    "    )\n",
    "    model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e6c35-0216-45c5-9f29-e2094095c08a",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9569cf-c2a3-4df8-a307-cbf766e8d6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c85a9-d4d2-43cf-9f6b-073e62fba431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../shopping_queries_dataset/'\n",
    "locale = 'us'\n",
    "model_path = model_save_path\n",
    "hypothesis_path_file = f\"./hypothesis/hypothesis_{locale}.csv\"\n",
    "batch_size = 256\n",
    "\n",
    "\"\"\" 0. Init variables \"\"\"\n",
    "col_query = \"query\"\n",
    "col_query_id = \"query_id\"\n",
    "col_product_id = \"product_id\" \n",
    "col_product_title = \"product_title\"\n",
    "col_product_locale = \"product_locale\"\n",
    "col_small_version = \"small_version\"\n",
    "col_split = \"split\"\n",
    "col_scores = \"scores\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dcf0926-3177-4f5c-8258-8411f34b04f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [03:04<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 1. Load data \"\"\"    \n",
    "df_examples = pd.read_parquet(os.path.join(dataset_path, 'shopping_queries_dataset_examples.parquet'))\n",
    "df_products = pd.read_parquet(os.path.join(dataset_path, 'shopping_queries_dataset_products.parquet'))\n",
    "\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=[col_product_locale, col_product_id],\n",
    "    right_on=[col_product_locale, col_product_id]\n",
    ")\n",
    "df_examples_products = df_examples_products[df_examples_products[col_small_version] == 1]\n",
    "df_examples_products = df_examples_products[df_examples_products[col_split] == \"test\"]\n",
    "df_examples_products = df_examples_products[df_examples_products[col_product_locale] == locale]\n",
    "df_examples_products[col_gain] = df_examples_products[col_esci_label].apply(lambda esci_label: esci_label2gain[esci_label])\n",
    "\n",
    "features_query = df_examples_products[col_query].to_list()\n",
    "features_product = df_examples_products[col_product_title].to_list()\n",
    "n_examples = len(features_query)\n",
    "scores = np.zeros(n_examples)\n",
    "\n",
    "if locale == \"us\":\n",
    "    \"\"\" 2. Prepare Cross-encoder model \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    \"\"\" 3. Generate hypothesis \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n_examples, batch_size)):\n",
    "            j = min(i + batch_size, n_examples)\n",
    "            features_query_ = features_query[i:j]\n",
    "            features_product_ = features_product[i:j]\n",
    "            features = tokenizer(features_query_, features_product_,  padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            scores[i:j] = np.squeeze(model(**features).logits.cpu().detach().numpy())\n",
    "            i = j\n",
    "else :\n",
    "    \"\"\" 2. Prepare Sentence transformer model \"\"\"\n",
    "    model = AutoModel.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    # CLS Pooling - Take output from first token\n",
    "    def cls_pooling(model_output):\n",
    "        return model_output.last_hidden_state[:,0]\n",
    "    # Encode text\n",
    "    def encode(texts):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input, return_dict=True)\n",
    "        # Perform pooling\n",
    "        embeddings = cls_pooling(model_output)\n",
    "        return embeddings\n",
    "    model.eval()\n",
    "\n",
    "    \"\"\" 3. Generate hypothesis \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n_examples, batch_size)):\n",
    "            j = min(i + batch_size, n_examples)\n",
    "            features_query_ = features_query[i:j]\n",
    "            features_product_ = features_product[i:j]\n",
    "            query_emb = encode(features_query_)\n",
    "            product_emb = encode(features_product_)\n",
    "            scores[i:j] = torch.diagonal(torch.mm(query_emb, product_emb.transpose(0, 1)).to('cpu'))\n",
    "            i = j\n",
    "\n",
    "\"\"\" 4. Prepare hypothesis file \"\"\"   \n",
    "df_hypothesis = pd.DataFrame({\n",
    "    col_query_id : df_examples_products[col_query_id].to_list(),\n",
    "    col_product_id : df_examples_products[col_product_id].to_list(),\n",
    "    col_esci_label : df_examples_products[col_esci_label].to_list(),\n",
    "    col_scores : scores,\n",
    "})\n",
    "df_hypothesis = df_hypothesis.sort_values(by=[col_query_id, col_scores], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e3e03c-90dc-4e63-83d7-d3f2c90d8996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> MRR for baseline models: \n",
      "\n",
      "{'scores': OrderedDict([('MRR', 0.8204),\n",
      "                        ('Hits@1', 0.7211),\n",
      "                        ('Hits@5', 0.9447),\n",
      "                        ('Hits@10', 0.9834)])}\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def calculate_metrics(df, col, hit_at_n=[1, 5, 10], pure_python=False):\n",
    "    \"\"\" Calculatye Metrics: MRR and Hits@n\n",
    "    It uses Ampligraph based on Tensorflow: https://docs.ampligraph.org/en/latest/index.html\n",
    "    If you prefer to do calculation based on pure Python, set pure_python=True\n",
    "    \"\"\"\n",
    "    result = OrderedDict()\n",
    "    df.loc[:, 'rank'] = df.groupby('query_id')[col].rank(method='min', ascending=False).values\n",
    "    first_hit_rank_position = df.groupby('query_id')[['gain', 'rank']] \\\n",
    "        .apply(lambda x: x[x.gain == 1.0]['rank'].min()).values\n",
    "    \n",
    "    first_hit_rank_position = np.nan_to_num(first_hit_rank_position, nan=1000)\n",
    "    \n",
    "    if not pure_python:\n",
    "        result[\"MRR\"] = mrr_score(first_hit_rank_position).round(4)\n",
    "        for h in hit_at_n:\n",
    "            result[f\"Hits@{h}\"] = hits_at_n_score(first_hit_rank_position, n=h).round(4)\n",
    "        \n",
    "    else:\n",
    "        n_queries = first_hit_rank_position.shape[0]\n",
    "        result[\"MRR\"] = np.divide(np.divide(1, first_hit_rank_position).sum(), \n",
    "                                  n_queries).round(4)\n",
    "        for h in hit_at_n:\n",
    "            result[f\"Hits@{h}\"] = np.divide((first_hit_rank_position <= h).sum(),\n",
    "                                            n_queries).round(4)\n",
    "    return result\n",
    "\n",
    "target_cols = ['scores']\n",
    "df_hypothesis[col_gain] = df_hypothesis[col_esci_label].apply(lambda esci_label: esci_label2gain[esci_label])\n",
    "print(f\"--> MRR for baseline models: \\n\")\n",
    "for col in target_cols:    \n",
    "    pprint({col: calculate_metrics(df_hypothesis, col)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125e730-4dc9-4e93-9402-02ccd79ee52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(root) Python *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
