{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c1a7b-b729-47a2-9ae7-838c84b8429d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install nltk streamlit sentencepiece textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e96c82-ced9-465c-bc29-1d094aba25a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 00:54:26.342290: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-13 00:54:26.409285: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/dell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/dell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ece748c2-553d-47e7-94c9-db7192f100f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../shopping_queries_dataset/'\n",
    "locale =\"us\"\n",
    "random_state = 42\n",
    "n_dev_queries = 200\n",
    "max_description_lenght = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebe1ea9-7752-4e8b-af9f-c7e3f7134454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "---------> cuda is activated <----------\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 0. Init variables \"\"\"\n",
    "col_query = \"query\"\n",
    "col_query_id = \"query_id\"\n",
    "col_product_id = \"product_id\" \n",
    "col_product_title = \"product_title\"\n",
    "col_product_description = \"product_description\"\n",
    "col_product_bullet_point = \"product_bullet_point\"\n",
    "col_product_brand = \"product_brand\"\n",
    "col_product_color = \"product_color\"\n",
    "col_product_locale = \"product_locale\"\n",
    "col_esci_label = \"esci_label\" \n",
    "col_small_version = \"small_version\"\n",
    "col_split = \"split\"\n",
    "col_gain = 'gain'\n",
    "col_features = [col_product_id]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "to_print = \"\".join(['-']*40)\n",
    "print(to_print)\n",
    "print(f\"---------> {device} is activated <----------\")\n",
    "print(to_print)\n",
    "esci_label2gain = {\n",
    "    'E' : 1.0,\n",
    "    'S' : 0.1,\n",
    "    'C' : 0.01,\n",
    "    'I' : 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d898e592-80e3-45aa-938f-cff87640bca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" 1. Load data \"\"\"    \n",
    "df_examples = pd.read_parquet(os.path.join(dataset_path, \n",
    "                                           'shopping_queries_dataset_examples.parquet'))\n",
    "df_products = pd.read_parquet(os.path.join(dataset_path, \n",
    "                                           'shopping_queries_dataset_products.parquet'))\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=[col_product_locale, col_product_id],\n",
    "    right_on=[col_product_locale, col_product_id]\n",
    ")\n",
    "df_examples_products = df_examples_products[df_examples_products[col_small_version] == 1]\n",
    "df_examples_products = df_examples_products[df_examples_products[col_product_locale] == locale]\n",
    "df_examples_products[col_gain] = \\\n",
    "    df_examples_products[col_esci_label].apply(lambda esci_label: esci_label2gain[esci_label])\n",
    "df_examples_products.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd51214-26fb-476e-809f-5d341d240d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_features = [col_query, col_product_title, col_product_description, \n",
    "                 col_product_bullet_point]\n",
    "categorical_features = [col_product_brand, col_product_color]\n",
    "# replacing null values with text as Unknown for feature processing\n",
    "df_examples_products[[col_product_description, col_product_bullet_point, \n",
    "                      col_product_brand, col_product_color]] = \\\n",
    "df_examples_products[[col_product_description, col_product_bullet_point, \n",
    "                      col_product_brand, col_product_color]].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee832a5-a5be-4ca6-9e86-fb34bae5cbfd",
   "metadata": {},
   "source": [
    "### Process Product Description Feature\n",
    "From the EDA step, the Product Description Feature is:\n",
    "- Format HTML. \n",
    "To make the feature ready, \n",
    "- Cleaning the feature with 'preprocess_html' function which is a simple cleaning approach for HTML text. For more advanced cleaning -Denoising, Normalization, Lemmatization, etc.- we rely on the LLM model that is being used to summarize text. In real-world use case, I would investigate both methods (classical cleaning methods and cleaning with the help of LLMs). \n",
    "- Summerize it with 'Falconsai/text_summarization' if its length is larger that 200 character to address the input size limit for LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77fafde9-564b-4403-9b93-36ec21837dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_html(html_text):\n",
    "    # Remove HTML tags\n",
    "    clean_text = BeautifulSoup(html_text, \"html.parser\").get_text(separator=\" \")\n",
    "    # Normalize whitespace\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    # Handle special characters\n",
    "    clean_text = clean_text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')\n",
    "    return clean_text\n",
    "df_examples_products[col_product_description] = \\\n",
    "    df_examples_products[col_product_description].apply(preprocess_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747e92b7-af71-4d39-a321-2da5a1df05d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://huggingface.co/Falconsai/text_summarization\n",
    "# Falconsai/text_summarization is a light LLM model for summarization tasks\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\",\n",
    "                      device=device)\n",
    "def remove_extra_spaces(text):\n",
    "    # Remove extra spaces using regular expression\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return re.sub(r'\\s+([.,!?])', r'\\1 ', text).strip()\n",
    "\n",
    "def suumarize_text(text):\n",
    "    if len(text) > 100:\n",
    "        text = remove_extra_spaces(\n",
    "            summarizer(text, max_length=max_description_lenght, min_length=100, \n",
    "                       do_sample=False)[0][\"summary_text\"]\n",
    "        )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "51160114-3e0c-408c-9bda-a75f6e31827c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for one batch of size 100 samples > min: 0 sec: 35\n"
     ]
    }
   ],
   "source": [
    "# Profiling Summarization with Falconsai/text_summarization\n",
    "from transformers.utils import logging\n",
    "import time\n",
    "\n",
    "\n",
    "logging.set_verbosity_error() \n",
    "\n",
    "batch_size = 100\n",
    "min_length = 20\n",
    "\n",
    "df_tmp = df_examples_products[col_product_description][0:batch_size]\n",
    "data = df_tmp[(df_tmp.apply(lambda x: len(x)) > max_description_lenght).values].to_list()\n",
    "\n",
    "tic = time.time()\n",
    "summarizer(data, max_length=max_description_lenght, min_length=min_length, \n",
    "           do_sample=False)[0][\"summary_text\"]\n",
    "minutes, seconds = divmod(time.time() - tic, 60)\n",
    "print(f\"Elapsed time for one batch of size {batch_size} samples > min: {int(minutes)} sec: {int(seconds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef1251-8b5f-440e-8c05-a0e5671c1762",
   "metadata": {},
   "source": [
    "Since summarizing will the LLM model is a time consuming approach, we pass for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2aaacbce-5288-4d98-8fd2-2b9dbf10d8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "\n",
    "def summarize(document, num_sentences=1):\n",
    "    # Tokenize the document into sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "    # Tokenize the document into words\n",
    "    words = word_tokenize(document.lower())\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Calculate the frequency of each word\n",
    "    freq = {}\n",
    "    for word in words:\n",
    "        if word not in freq:\n",
    "            freq[word] = 0\n",
    "        freq[word] += 1\n",
    "    # Calculate the score of each sentence\n",
    "    scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in freq:\n",
    "                if sentence not in scores:\n",
    "                    scores[sentence] = 0\n",
    "                scores[sentence] += freq[word]\n",
    "    # Rank the sentences by score\n",
    "    ranked_sentences = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Select the top ranked sentences\n",
    "    summary = ' '.join([sentence for sentence, score in ranked_sentences[:num_sentences]])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3afad5ab-b91d-4e3c-8abf-42d10264bc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_examples_products[col_product_description] = df_examples_products[col_product_description].apply(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4bdc7d4a-c88f-4d76-8906-b1ee8c3c0c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./processed_product_description.csv\"):\n",
    "    df_examples_products.to_csv(\"./processed_product_description.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e100b-f12f-42b0-94d5-c0f963a5ceae",
   "metadata": {},
   "source": [
    "### Process Product Bullet Points Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4cb7dd1c-db3f-4207-bd8f-3c1e52188421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_itemized_text(text):\n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Replace bullet points with newline characters\n",
    "    text = re.sub(r'^[\\s]*\\*[\\s]*', '\\n', text, flags=re.MULTILINE)\n",
    "    # Remove excess newline characters\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    # Remove leading and trailing whitespace from each line\n",
    "    text = re.sub(r'^[\\s]*|[\\s]*$', '', text, flags=re.MULTILINE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e3416e1-2e5c-4b95-8d9c-f20c0e932177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_examples_products[col_product_bullet_point] = \\\n",
    "    df_examples_products[col_product_bullet_point].apply(\n",
    "        lambda x: summarize(clean_itemized_text(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e0bef-66f2-4e4a-98e8-0b0b56897e6c",
   "metadata": {},
   "source": [
    "# Combining Categorical Features with Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a8e57-9dd2-4d3e-8da9-05ecabbfb21b",
   "metadata": {},
   "source": [
    "To get the most out of all the features, one approach is combining categorical features with test features and adding some context to it. For instance:\n",
    "- The product brand is \\<x> or simply Brand: \\<x>\n",
    "For some models like BERT, it can be adding special tokens like \\[SEP\\] between features. This saves significantly on the sequence length while preserving the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c705f19a-a70b-4aaf-a356-b1927727b785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine all features into string\n",
    "\n",
    "def combine_features(row):\n",
    "    combined = \"\"\n",
    "    combined += f\"Brand: {row[col_product_brand]}, \" \\\n",
    "                f\"Color: {row[col_product_color]}, \" \\\n",
    "                f\"Product Title: {row[col_product_title]}, \" \\\n",
    "                f\"Product Description: {row[col_product_description]}, and \" \\\n",
    "                f\"Product Features: {row[col_product_bullet_point]}.\"\n",
    "    return combined\n",
    "        \n",
    "combined_features = []\n",
    "for (_, row) in df_examples_products.iterrows():\n",
    "    combined_features.append(combine_features(row))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "36f6e689-e2dd-4659-9aa8-76a2d4012b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_examples_products[\"combined_features\"] = combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e6c277c-2a2d-41dc-ae8f-dd381bb081bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./final_features.csv\"):\n",
    "    df_examples_products.to_csv(\"./final_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0cac5-7a89-4330-b788-cf951ccefce2",
   "metadata": {
    "tags": []
   },
   "source": [
    "All toghther, the feature engineering processes are time consuming and we continue with just the main feature **Product Title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "29fd267f-c6ba-4bb4-91d7-fe2f15672a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Brand: RamPro, Color: 10 Inch, Product Title: ...\n",
       "1         Brand: MaxAuto, Color: Unknown, Product Title:...\n",
       "2         Brand: Neiko, Color: Unknown, Product Title: N...\n",
       "3         Brand: Russo, Color: Unknown, Product Title: 2...\n",
       "4         Brand: Antego Tire & Wheel, Color: Husqvarna S...\n",
       "                                ...                        \n",
       "601349    Brand: Nilight, Color: Unknown, Product Title:...\n",
       "601350    Brand: Burley Design, Color: Red, Product Titl...\n",
       "601351    Brand: Burley Design, Color: Yellow, Product T...\n",
       "601352    Brand: BELL, Color: 20\"x1.75-2.25\" Schrader, P...\n",
       "601353    Brand: Marcy, Color: Black/Gray/Copper, Produc...\n",
       "Name: combined_features, Length: 601354, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_examples_products[\"combined_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8ec99-99ef-45a6-a7ba-4b50d9de21d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(root) Python *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
