{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e451673-6ffc-4295-aeb3-84f8333d5ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifacts.dell.com/artifactory/api/pypi/python/simple, https://artifacts.dell.com/artifactory/api/pypi/ailfc-1003745-pypi-prd-local/simple, https://artifacts.dell.com/artifactory/api/pypi/aia-1001238-pypi-prd-local/simple, https://artifacts.dell.com/artifactory/api/pypi/aiops-1002685-pypi-prd-local/simple\n",
      "Requirement already satisfied: faiss-cpu in /opt/conda/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /home/dell/.local/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202a8046-5da0-47fe-b256-c805ce725aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers import evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a37c7d-a1db-43e3-a097-4eb147475418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../shopping_queries_dataset/'\n",
    "locale =\"us\"\n",
    "model_save_path = f\"./models_{locale}\"\n",
    "output_path = f\"{model_save_path}_training\"\n",
    "random_state = 42\n",
    "n_dev_queries = 200\n",
    "train_batch_size = 32\n",
    "train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d924736-b6e3-4094-ac77-e0385de7e8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "---------> cuda is activated <----------\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 0. Init variables \"\"\"\n",
    "col_query = \"query\"\n",
    "col_query_id = \"query_id\"\n",
    "col_product_id = \"product_id\" \n",
    "col_product_title = \"product_title\"\n",
    "col_product_locale = \"product_locale\"\n",
    "col_esci_label = \"esci_label\" \n",
    "col_small_version = \"small_version\"\n",
    "col_split = \"split\"\n",
    "col_gain = 'gain'\n",
    "col_features = [col_product_id]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "to_print = \"\".join(['-']*40)\n",
    "print(to_print)\n",
    "print(f\"---------> {device} is activated <----------\")\n",
    "print(to_print)\n",
    "esci_label2gain = {\n",
    "    'E' : 1.0,\n",
    "    'S' : 0.1,\n",
    "    'C' : 0.01,\n",
    "    'I' : 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a54d173-aff3-4782-a8b1-130dfef7148c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" 1. Load data \"\"\"    \n",
    "df_examples = pd.read_parquet(os.path.join(dataset_path, 'shopping_queries_dataset_examples.parquet'))\n",
    "df_products = pd.read_parquet(os.path.join(dataset_path, 'shopping_queries_dataset_products.parquet'))\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=[col_product_locale, col_product_id],\n",
    "    right_on=[col_product_locale, col_product_id]\n",
    ")\n",
    "df_examples_products = df_examples_products[df_examples_products[col_small_version] == 1]\n",
    "df_examples_products = df_examples_products[df_examples_products[col_product_locale] == locale]\n",
    "df_examples_products[col_gain] = df_examples_products[col_esci_label].apply(lambda esci_label: esci_label2gain[esci_label])\n",
    "\n",
    "df_train = df_examples_products[[col_query_id, col_query, *col_features, col_gain]][df_examples_products[col_split] == \"train\"]\n",
    "list_query_id = df_train[col_query_id].unique()\n",
    "dev_size = n_dev_queries / len(list_query_id)\n",
    "list_query_id_train, list_query_id_dev = train_test_split(list_query_id, test_size=dev_size, random_state=random_state)\n",
    "\n",
    "df_train = df_examples_products[df_examples_products[col_query_id].isin(list_query_id_train)]\n",
    "df_dev = df_examples_products[df_examples_products[col_query_id].isin(list_query_id_dev)]\n",
    "df_test = df_examples_products[df_examples_products[col_split] == \"test\"]\n",
    "\n",
    "# This part of the code is for indexing and it is assumed the only input feature is product_title.\n",
    "# Otherwise it shoudl be updated asccordingly\n",
    "id_features_product_test = df_test[[col_product_id, col_product_title]].drop_duplicates(subset=col_product_title)\n",
    "\n",
    "features_product_test = id_features_product_test[col_product_title].to_list()\n",
    "id_product_test = id_features_product_test[col_product_id].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab90014-92c4-40c7-baf8-281c9040262e",
   "metadata": {},
   "source": [
    "# Training Retrieval Bi-encoder Models and Indexing with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f2304-6a29-485e-948a-224680125b62",
   "metadata": {},
   "source": [
    "## Retrieval Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cfd50d1-4df1-44fd-8931-c7a142065519",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    \"\"\" Prepare data loaders \"\"\"\n",
    "    train_samples = []\n",
    "    for (_, row) in df_train.iterrows():\n",
    "        train_samples.append(InputExample(texts=[row[col_query], row[col_product_title]], label=float(row[col_gain])))\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size, drop_last=True)\n",
    "\n",
    "    dev_queries = df_dev[col_query].to_list()\n",
    "    dev_titles = df_dev[col_product_title].to_list()\n",
    "    dev_scores = df_dev[col_gain].to_list()   \n",
    "    evaluator = evaluation.EmbeddingSimilarityEvaluator(dev_queries, dev_titles, dev_scores)\n",
    "\n",
    "    \"\"\" Prepare sentence transformers model: \n",
    "        https://www.sbert.net/docs/training/overview.html \n",
    "    \"\"\"\n",
    "    model_names = [\n",
    "        'sentence-transformers/multi-qa-mpnet-base-dot-v1', # specific for semantic search\n",
    "        'sentence-transformers/all-mpnet-base-v2' # general purpose model\n",
    "    ]\n",
    "\n",
    "    for model_name in model_names:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "        num_epochs = 1\n",
    "        evaluation_steps = 1000\n",
    "        \"\"\" 4. Train Sentence transformer model \"\"\"\n",
    "        model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            evaluator=evaluator,\n",
    "            epochs=num_epochs,\n",
    "            evaluation_steps=evaluation_steps,\n",
    "            output_path=f\"{output_path}_retrieval_{model_name}\",\n",
    "        )\n",
    "        model.save(f\"{model_save_path}_retrieval_{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69cd69d-15d7-4ac7-8dc2-979b0d0680de",
   "metadata": {},
   "source": [
    "## Retrieval Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cc292d-10cf-4fa0-8a21-6a030f96f3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieval_inference(model_path, text=None, batch_scoring=False, query_result_pair=None, batch_size=256):\n",
    "    \"\"\" Embeddings for the trained bi-encoder models \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModel.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    # CLS Pooling - Take output from first token\n",
    "    def cls_pooling(model_output):\n",
    "        return model_output.last_hidden_state[:,0]\n",
    "    # Encode text\n",
    "    def encode(texts):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input, return_dict=True)\n",
    "        # Perform pooling\n",
    "        embeddings = cls_pooling(model_output)\n",
    "        return embeddings\n",
    "    model.eval()\n",
    "    if not batch_scoring:\n",
    "        return encode(text)\n",
    "    features_query, features_product = query_result_pair\n",
    "    n_examples = len(features_query)\n",
    "    scores = np.zeros(n_examples)\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n_examples, batch_size)):\n",
    "            j = min(i + batch_size, n_examples)\n",
    "            features_query_ = features_query[i:j]\n",
    "            features_product_ = features_product[i:j]\n",
    "            query_emb = encode(features_query_)\n",
    "            product_emb = encode(features_product_)\n",
    "            scores[i:j] = torch.diagonal(torch.mm(query_emb, product_emb.transpose(0, 1)).to('cpu'))\n",
    "            i = j\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08dc5c55-8ad8-4f86-b364-74c71dcaefd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0909, -0.3888, -0.1402,  ..., -0.3611,  0.3270, -0.5371],\n",
       "        [-0.1767,  0.0056, -0.2427,  ..., -0.3935,  0.0651, -0.4160],\n",
       "        [-0.1198, -0.2018, -0.2021,  ..., -0.3449,  0.1465, -0.3667],\n",
       "        [-0.3555, -0.5554, -0.2343,  ...,  0.0134,  0.1727, -0.4320],\n",
       "        [-0.2690, -0.6249, -0.2034,  ..., -0.0024,  0.0924, -0.3410]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './models_us_retrieval_sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "retrieval_inference(model_path, features_product_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b538c1dc-a46f-42ad-a29b-978bf193d10b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14.44833279, 14.23288059, 15.44969654, 16.10173988, 15.30306435,\n",
       "        4.43290424,  8.929492  , 15.40648079, 12.94305611, 15.44424057,\n",
       "       15.81794357, 18.69197273,  7.63707542, 14.20569992, 12.38149071,\n",
       "       12.53182888, 13.94957447,  1.99389601,  5.57536268, 18.19939423,\n",
       "        6.5950985 ,  5.41637468, 10.56408119, 25.58846283, 26.34155083,\n",
       "       17.76875305, 15.93656158,  5.68320227, 15.00357914,  6.81787443])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './models_us_retrieval_sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "retrieval_inference(model_path, batch_scoring=True, \n",
    "                    query_result_pair=(df_test[col_query].to_list()[0:30],\n",
    "                                       df_test[col_product_title].to_list()[0:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0197049-8099-4f36-b3bc-6b052a2ebc5f",
   "metadata": {},
   "source": [
    "# Indexing with FAISS \n",
    "\n",
    "Note: Just indexing test set to save time and for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eff4ecf-1a4e-4fbc-a4dc-7cc6f49baedb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index file exist ./us_multi-qa-mpnet-base-dot-v1.index\n"
     ]
    }
   ],
   "source": [
    "def indexing_faiss(list_to_index, model_path, index_file_name, batch_size=256):\n",
    "    embedding_size = 768\n",
    "    n_examples = len(list_to_index)\n",
    "    index = faiss.IndexIDMap(faiss.IndexFlatIP(embedding_size))\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n_examples, batch_size)):\n",
    "            j = min(i + batch_size, n_examples)\n",
    "            list_to_index_ = list_to_index[i:j]\n",
    "            index.add_with_ids(\n",
    "                retrieval_inference(model_path=model_path, text=list_to_index_) \\\n",
    "                    .to('cpu').numpy().astype('float32'), \n",
    "                np.array(range(i, j))\n",
    "            )\n",
    "    assert index.ntotal == n_examples, \"Not all the inputs are indexed\"\n",
    "    faiss.write_index(index, index_file_name)\n",
    "\n",
    "model_path = './models_us_retrieval_sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "\n",
    "def global_index_file_name(model_path, locale):\n",
    "    if locale: return f\"./{locale}_{model_path.split('/')[-1]}.index\"\n",
    "    return f\"./{model_path.split('/')[-1]}.index\"\n",
    "\n",
    "index_file_name = f\"./{locale}_{model_path.split('/')[-1]}.index\"\n",
    "if not os.path.isfile(index_file_name):\n",
    "    indexing_faiss(list_to_index=features_product_test, \n",
    "                   model_path=model_path,\n",
    "                   index_file_name=f\"./{locale}_{model_path.split('/')[-1]}.index\", \n",
    "                   batch_size=256\n",
    "                  )\n",
    "else:\n",
    "    print(f\"The index file exist {index_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81320dd0-1ee8-44ec-ada1-032e6ec1ef46",
   "metadata": {},
   "source": [
    "## Inference Retrieval-Indexing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "677b5fb7-d506-43b7-8c4d-35dac927caaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in Total Time: 1.3086068630218506\n",
      "{'query': '!qscreen fence without holes',\n",
      " 'retrieval results': [{'Product ID': '1933054395',\n",
      "                        'Product Title': 'Qwirkle Board Game'},\n",
      "                       {'Product ID': 'B018UP8Z26',\n",
      "                        'Product Title': 'QMX Desktop Model Hoverboard'},\n",
      "                       {'Product ID': 'B018JYCD9Y',\n",
      "                        'Product Title': 'Zippity Outdoor Products ZP19002 No '\n",
      "                                         'Dig Fence Newport, 36\"H x 72\"W, '\n",
      "                                         'White'},\n",
      "                       {'Product ID': 'B001OJXVKW',\n",
      "                        'Product Title': 'Windscreen4less Heavy Duty Privacy '\n",
      "                                         \"Screen Fence in Color Solid Black 6' \"\n",
      "                                         \"x 50' Brass Grommets 150 GSM - \"\n",
      "                                         'Customized'},\n",
      "                       {'Product ID': 'B07QZ8BXVB',\n",
      "                        'Product Title': 'Zippity Outdoor Products ZP19037 No '\n",
      "                                         'Dig Baskenridge Semi-Permanent Vinyl '\n",
      "                                         'Fence, White (36in H x 42in W)- 2 '\n",
      "                                         'pack'}]}\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from pprint import pprint\n",
    "\n",
    "model_path = './models_us_retrieval_sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "\n",
    "def fetch_id_product(indices):\n",
    "    return [{\"Product ID\": id_product_test[i], \"Product Title\": features_product_test[i]} for i in indices]\n",
    "\n",
    "def retriev(query, top_k=5, locale=\"us\", model_path=model_path):\n",
    "    index = faiss.read_index(global_index_file_name(model_path, locale))\n",
    "    tick = time.time()\n",
    "    query_vector = retrieval_inference(model_path, query).to('cpu').numpy().astype('float32')\n",
    "    top_k = index.search(query_vector, top_k)\n",
    "    print(f\"Results in Total Time: {time.time() - tick}\")\n",
    "    top_k_ids = top_k[1].tolist()[0]\n",
    "    return fetch_id_product(top_k_ids)\n",
    "\n",
    "query = df_test[col_query].iloc[0]\n",
    "pprint({\"query\": query, \"retrieval results\": retriev(query)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5941eb19-fd3d-4bd8-aa3e-2281cd695566",
   "metadata": {},
   "source": [
    "# Training Re-Ranking Cross-Encoder Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595a2aa-0518-4a81-962d-532d60bf06a8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f2b7b01-1395-4dde-a010-21a973c9159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    \"\"\" Prepare data loaders \"\"\"\n",
    "    train_samples = []\n",
    "    for (_, row) in df_train.iterrows():\n",
    "        train_samples.append(InputExample(texts=[row[col_query], row[col_product_title]], label=float(row[col_gain])))\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size, drop_last=True)\n",
    "\n",
    "    dev_samples = {}\n",
    "    query2id = {}\n",
    "    for (_, row) in df_dev.iterrows():\n",
    "        try:\n",
    "            qid = query2id[row[col_query]]\n",
    "        except KeyError:\n",
    "            qid = len(query2id)\n",
    "            query2id[row[col_query]] = qid\n",
    "        if qid not in dev_samples:\n",
    "            dev_samples[qid] = {'query': row[col_query], 'positive': set(), 'negative': set()}\n",
    "        if row[col_gain] > 0:\n",
    "            dev_samples[qid]['positive'].add(row[col_product_title])\n",
    "        else:\n",
    "            dev_samples[qid]['negative'].add(row[col_product_title])\n",
    "    evaluator = CERerankingEvaluator(dev_samples, name='train-eval')\n",
    "\n",
    "    \"\"\" Prepare Cross-enconder model:\n",
    "        https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/ms_marco/train_cross-encoder_kd.py\n",
    "    \"\"\"\n",
    "    model_names = ['cross-encoder/ms-marco-MiniLM-L-12-v2', 'cross-encoder/stsb-roberta-large']\n",
    "    num_epochs = 1\n",
    "    num_labels = 1\n",
    "    max_length = 512\n",
    "    default_activation_function = torch.nn.Identity()\n",
    "\n",
    "    for model_name in model_names:\n",
    "        model = CrossEncoder(\n",
    "            model_name, \n",
    "            num_labels=num_labels, \n",
    "            max_length=max_length, \n",
    "            default_activation_function=default_activation_function, \n",
    "            device=device\n",
    "        )\n",
    "        loss_fct=torch.nn.MSELoss()\n",
    "        evaluation_steps = 5000\n",
    "        warmup_steps = 5000\n",
    "        lr = 7e-6\n",
    "        \"\"\" Train Cross-encoder model \"\"\"\n",
    "        model.fit(\n",
    "            train_dataloader=train_dataloader,\n",
    "            loss_fct=loss_fct,\n",
    "            evaluator=evaluator,\n",
    "            epochs=num_epochs,\n",
    "            evaluation_steps=evaluation_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            output_path=f\"{output_path}_reranking_{model_name}\",\n",
    "            optimizer_params={'lr': lr},\n",
    "        )\n",
    "        model.save(f\"{model_save_path}_reranking_{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815fed6-8229-436e-9a1a-68a9c6ff2034",
   "metadata": {},
   "source": [
    "## Re-Ranking Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4275e9f-db5e-42ad-9c92-360062595433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reranking_inference(model_path, features_query, features_product, batch_size=256):\n",
    "    \"\"\" Scoring for the trained cross-encoder models \"\"\"\n",
    "    n_examples = len(features_query)\n",
    "    scores = np.zeros(n_examples)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n_examples, batch_size)):\n",
    "            j = min(i + batch_size, n_examples)\n",
    "            features_query_ = features_query[i:j]\n",
    "            features_product_ = features_product[i:j]\n",
    "            features = tokenizer(features_query_, features_product_, \n",
    "                                 padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            scores[i:j] = np.squeeze(model(**features).logits.cpu().detach().numpy())\n",
    "            i = j\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caaeefd7-9c2c-40f5-8451-87a5304c0011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.09063359, 0.21230277, 0.32579595, 0.32453728, 0.31910166,\n",
       "       0.05987772, 0.29635704, 0.40455851, 0.23366484, 0.40120405])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './models_us_reranking_cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "reranking_inference(model_path, \n",
    "                    features_query=df_test[col_query].to_list()[0:10], \n",
    "                    features_product=features_product_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e3716-72f5-4a06-b157-004777f18fa5",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1f269-1a1c-482a-8995-87ee1dac6930",
   "metadata": {},
   "source": [
    "## Trained Performance of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b42e94e-2809-454e-8176-b19d87c8722e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [09:50<00:00,  1.20it/s]\n",
      "/tmp/ipykernel_1392/3087869044.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.loc[:, f\"retrieval_{retrieval_model_path.split('/')[-1]}\"] = scores.copy()\n",
      "100%|██████████| 710/710 [09:49<00:00,  1.20it/s]\n",
      "/tmp/ipykernel_1392/3087869044.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.loc[:, f\"retrieval_{retrieval_model_path.split('/')[-1]}\"] = scores.copy()\n",
      "100%|██████████| 710/710 [03:04<00:00,  3.86it/s]\n",
      "/tmp/ipykernel_1392/3087869044.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.loc[:, f\"ranking_{ranking_model_path.split('/')[-1]}\"] = scores.copy()\n"
     ]
    }
   ],
   "source": [
    "features_query = df_test[col_query]\n",
    "features_products = df_test[col_product_title]\n",
    "\n",
    "retrieval_model_paths = [\n",
    "    './models_us_training_retrieval_sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
    "    './models_us_training_retrieval_sentence-transformers/all-mpnet-base-v2'\n",
    "]\n",
    "ranking_model_paths = [\n",
    "    './models_us_training_reranking_cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "    # './models_us_training_reranking_cross-encoder/stsb-roberta-large'\n",
    "]\n",
    "\n",
    "for retrieval_model_path in retrieval_model_paths:\n",
    "    scores = retrieval_inference(retrieval_model_path, batch_scoring=True, \n",
    "                                 query_result_pair=(features_query.to_list(), \n",
    "                                                    features_products.to_list()))\n",
    "    df_test.loc[:, f\"retrieval_{retrieval_model_path.split('/')[-1]}\"] = scores.copy()\n",
    "    \n",
    "for ranking_model_path in ranking_model_paths:     \n",
    "    scores = reranking_inference(ranking_model_path, \n",
    "                                 features_query.to_list(), \n",
    "                                 features_products.to_list())\n",
    "    df_test.loc[:, f\"ranking_{ranking_model_path.split('/')[-1]}\"] = scores.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a421c3d-40df-4f0c-a085-230b198f17e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_csv(\"./scores_from_trained_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d36c9cc-41e6-42a1-acd6-66c53e696122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieval_multi-qa-mpnet-base-dot-v1</th>\n",
       "      <th>retrieval_all-mpnet-base-v2</th>\n",
       "      <th>ranking_ms-marco-MiniLM-L-12-v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16.829475</td>\n",
       "      <td>3.194620</td>\n",
       "      <td>0.224360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20.914606</td>\n",
       "      <td>3.413810</td>\n",
       "      <td>0.315618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>21.134993</td>\n",
       "      <td>3.965578</td>\n",
       "      <td>0.383481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>19.273392</td>\n",
       "      <td>3.752367</td>\n",
       "      <td>0.412195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>19.473473</td>\n",
       "      <td>3.713756</td>\n",
       "      <td>0.411183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614589</th>\n",
       "      <td>11.579144</td>\n",
       "      <td>3.313579</td>\n",
       "      <td>0.142542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614590</th>\n",
       "      <td>12.583954</td>\n",
       "      <td>3.255656</td>\n",
       "      <td>0.187810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614591</th>\n",
       "      <td>9.215116</td>\n",
       "      <td>2.284095</td>\n",
       "      <td>0.280399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614592</th>\n",
       "      <td>15.037724</td>\n",
       "      <td>3.381042</td>\n",
       "      <td>0.189536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614593</th>\n",
       "      <td>14.783213</td>\n",
       "      <td>3.175278</td>\n",
       "      <td>0.183821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181701 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         retrieval_multi-qa-mpnet-base-dot-v1  retrieval_all-mpnet-base-v2  \\\n",
       "32                                  16.829475                     3.194620   \n",
       "33                                  20.914606                     3.413810   \n",
       "34                                  21.134993                     3.965578   \n",
       "35                                  19.273392                     3.752367   \n",
       "36                                  19.473473                     3.713756   \n",
       "...                                       ...                          ...   \n",
       "2614589                             11.579144                     3.313579   \n",
       "2614590                             12.583954                     3.255656   \n",
       "2614591                              9.215116                     2.284095   \n",
       "2614592                             15.037724                     3.381042   \n",
       "2614593                             14.783213                     3.175278   \n",
       "\n",
       "         ranking_ms-marco-MiniLM-L-12-v2  \n",
       "32                              0.224360  \n",
       "33                              0.315618  \n",
       "34                              0.383481  \n",
       "35                              0.412195  \n",
       "36                              0.411183  \n",
       "...                                  ...  \n",
       "2614589                         0.142542  \n",
       "2614590                         0.187810  \n",
       "2614591                         0.280399  \n",
       "2614592                         0.189536  \n",
       "2614593                         0.183821  \n",
       "\n",
       "[181701 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['retrieval_multi-qa-mpnet-base-dot-v1',\n",
    "        'retrieval_all-mpnet-base-v2',\n",
    "        'ranking_ms-marco-MiniLM-L-12-v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1559a810-4f4a-403a-9aff-cb5069eb46b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install ampligraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d2a88a5-07b0-417d-ac0d-7b6ea3b76037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> MRR for trained models: \n",
      "\n",
      "{'retrieval_multi-qa-mpnet-base-dot-v1': OrderedDict([('MRR', 0.803),\n",
      "                                                      ('Hits@1', 0.697),\n",
      "                                                      ('Hits@5', 0.9395),\n",
      "                                                      ('Hits@10', 0.9797)])}\n",
      "{'retrieval_all-mpnet-base-v2': OrderedDict([('MRR', 0.7943),\n",
      "                                             ('Hits@1', 0.6812),\n",
      "                                             ('Hits@5', 0.9378),\n",
      "                                             ('Hits@10', 0.9817)])}\n",
      "{'ranking_ms-marco-MiniLM-L-12-v2': OrderedDict([('MRR', 0.8109),\n",
      "                                                 ('Hits@1', 0.7067),\n",
      "                                                 ('Hits@5', 0.9442),\n",
      "                                                 ('Hits@10', 0.9825)])}\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def calculate_metrics(df, col, hit_at_n=[1, 5, 10], pure_python=False):\n",
    "    \"\"\" Calculatye Metrics: MRR and Hits@n\n",
    "    It uses Ampligraph based on Tensorflow: https://docs.ampligraph.org/en/latest/index.html\n",
    "    If you prefer to do calculation based on pure Python, set pure_python=True\n",
    "    \"\"\"\n",
    "    result = OrderedDict()\n",
    "    df.loc[:, 'rank'] = df.groupby('query_id')[col].rank(method='min', ascending=False).values\n",
    "    first_hit_rank_position = df.groupby('query_id')[['gain', 'rank']] \\\n",
    "        .apply(lambda x: x[x.gain == 1.0]['rank'].min()).values\n",
    "    \n",
    "    first_hit_rank_position = np.nan_to_num(first_hit_rank_position, nan=1000)\n",
    "    \n",
    "    if not pure_python:\n",
    "        result[\"MRR\"] = mrr_score(first_hit_rank_position).round(4)\n",
    "        for h in hit_at_n:\n",
    "            result[f\"Hits@{h}\"] = hits_at_n_score(first_hit_rank_position, n=h).round(4)\n",
    "        \n",
    "    else:\n",
    "        n_queries = first_hit_rank_position.shape[0]\n",
    "        result[\"MRR\"] = np.divide(np.divide(1, first_hit_rank_position).sum(), \n",
    "                                  n_queries).round(4)\n",
    "        for h in hit_at_n:\n",
    "            result[f\"Hits@{h}\"] = np.divide((first_hit_rank_position <= h).sum(),\n",
    "                                            n_queries).round(4)\n",
    "    return result\n",
    "\n",
    "target_cols = ['retrieval_multi-qa-mpnet-base-dot-v1',\n",
    "               'retrieval_all-mpnet-base-v2',\n",
    "               'ranking_ms-marco-MiniLM-L-12-v2']\n",
    "metrics = OrderedDict()\n",
    "print(f\"--> MRR for trained models: \\n\")\n",
    "for col in target_cols:    \n",
    "    pprint({col: calculate_metrics(df_test, col)})\n",
    "    metrics[col] = calculate_metrics(df_test, col).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9cd76-db62-4517-8bf3-0c80c3630771",
   "metadata": {},
   "source": [
    "## End-to-End System Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c74cb631-55f8-42b7-bddc-2f48f3afe6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Cleaning GPU \n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c0d00d0-ed97-435c-9ff9-7cb8a9fc9f78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8956"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[col_query].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501feca-fc62-41a3-b4f8-cb2c81dbcc13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [13:21<4:12:29, 159.47s/it]"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from pprint import pprint\n",
    "\n",
    "# Selected Retrieval Model\n",
    "model_path = './models_us_retrieval_sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "n_batches=100\n",
    "batch_size=80\n",
    "top_k=30\n",
    "\n",
    "def fetch_id_product(row, indices):\n",
    "    return [{col_query_id: row[col_query_id],\n",
    "             col_query: row[col_query],\n",
    "             col_product_id: id_product_test[i], \n",
    "             col_product_title: features_product_test[i]} for i in indices]\n",
    "\n",
    "def retriev(row, top_k=5, locale=\"us\", model_path=model_path):\n",
    "    index = faiss.read_index(global_index_file_name(model_path, locale))\n",
    "    query_vector = retrieval_inference(model_path, row[col_query]).to('cpu').numpy().astype('float32')\n",
    "    top_k = index.search(query_vector, top_k)\n",
    "    top_k_ids = top_k[1].tolist()[0]\n",
    "    return fetch_id_product(row, top_k_ids)\n",
    "\n",
    "def sampling_retrieval(model_path, df_queries, n_batches=n_batches, batch_size=batch_size, top_k=top_k):\n",
    "    result = []\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        features_queries = df_queries.sample(n=batch_size) # default replacement is False\n",
    "        for (_, row) in features_queries.iterrows():\n",
    "            result.append(retriev(row, top_k=top_k, locale=\"us\", model_path=model_path))       \n",
    "    return result\n",
    "\n",
    "df_queries = df_test[[col_query_id, col_query, col_gain]].drop_duplicates()\n",
    "result = sampling_retrieval(model_path, df_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b418b1-61a4-461a-93b9-3e0a0ac49cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def flatten_chain(matrix):\n",
    "    return list(chain.from_iterable(matrix))\n",
    "\n",
    "ranking_model_path = './models_us_training_reranking_cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "col = 'ranking_ms-marco-MiniLM-L-12-v2'\n",
    "sample_size = 50 \n",
    "sample_result = []\n",
    "n_iterations = 1000\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    df_ = pd.DataFrame(flatten_chain(random.sample(result, sample_size)))\n",
    "    scores = reranking_inference(ranking_model_path, \n",
    "                                 df_[col_query].to_list(), \n",
    "                                 df_[col_product_title].to_list())\n",
    "    score_col = f\"ranking_{ranking_model_path.split('/')[-1]}\"\n",
    "    df_.loc[:, score_col] = scores.copy()\n",
    "    df_.loc[:, \"rank\"] = df_.groupby(col_query_id)[score_col].rank(method='min', ascending=False).values\n",
    "    df_ = df_[df_['rank'] <= 10]\n",
    "    df_rank = df_test[df_test[col_query_id].isin(df_[col_query_id].unique())][[col_query_id, col_product_id, col_gain]].merge(\n",
    "        df_,\n",
    "        how='left',\n",
    "        on=[col_query_id, col_product_id]\n",
    "    )\n",
    "    df_rank.loc[:, 'rank'] = df_rank.groupby('query_id')[col]. \\\n",
    "    rank(method='min', ascending=False).values\n",
    "    df_rank['ranking_ms-marco-MiniLM-L-12-v2'] = df_rank['ranking_ms-marco-MiniLM-L-12-v2'].fillna(0)\n",
    "    sample_result.append(calculate_metrics(df_rank, col).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1436234-9add-44a6-8fce-a40b4799e9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(sample_result).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e934c1b-e4a5-4d74-8c3e-40c7830a8bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(root) Python *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
